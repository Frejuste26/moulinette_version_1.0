import os
import uuid
import logging
from datetime import datetime, timedelta
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import pandas as pd
import json

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/inventory_processor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Imports des services
from config import config
from services.file_processor import FileProcessorService
from services.session_service import SessionService
from services.lotecart_processor import LotecartProcessor
from utils.validators import FileValidator
from utils.error_handler import handle_api_errors, APIErrorHandler
from utils.rate_limiter import apply_rate_limit
from database import db_manager

# Initialisation Flask
app = Flask(__name__)
# CORS configur√© via YAML
try:
    from services.config_service import config_service as _cfg
    _cors_cfg = _cfg.get_cors_config()
    if _cors_cfg.get('enabled', True):
        _allowed = _cors_cfg.get('allowed_origins', ["*"])
        CORS(
            app,
            resources={r"/api/*": {"origins": _allowed}},
            origins=_allowed,
            expose_headers=["Content-Disposition"],
            supports_credentials=False
        )
    else:
        CORS(app, resources={})
except Exception:
    CORS(app)

# Configuration
app.config['MAX_CONTENT_LENGTH'] = config.MAX_FILE_SIZE
app.config['SECRET_KEY'] = config.SECRET_KEY

# Services
file_processor = FileProcessorService()
session_service = SessionService()
lotecart_processor = LotecartProcessor()

class InventoryProcessor:
    """Processeur principal pour les inventaires Sage X3"""
    
    def __init__(self):
        self.sessions = {}  # Stockage temporaire en m√©moire (sera migr√© vers DB)
        logger.info("InventoryProcessor initialis√©")
        # Param√®tres de formatage pour les nombres d√©cimaux
        self._decimal_precision = 6  # pr√©cision par d√©faut, ajustable si besoin
        self._zero_epsilon = 1e-9    # tol√©rance z√©ro pour comparaisons flottantes

    def _format_number(self, value) -> str:
        """Formate un nombre (int/float/str) en cha√Æne sans perte inutile pour supporter les d√©cimaux.
        - Garde les d√©cimales si pr√©sentes
        - Supprime les z√©ros et le point de fin inutiles
        - Accepte d√©j√† des cha√Ænes et les retourne telles quelles (apr√®s trim)
        """
        try:
            # Si d√©j√† une cha√Æne, renvoyer nettoy√©e
            if isinstance(value, str):
                s = value.strip()
                # normaliser les virgules √©ventuelles en points si pr√©sentes
                s = s.replace(',', '.')
                try:
                    f = float(s)
                except Exception:
                    return s
            else:
                f = float(value)

            # Utiliser une pr√©cision raisonnable puis strip des z√©ros
            fmt = f"{f:.{self._decimal_precision}f}"
            # supprimer z√©ros et point superflus
            if '.' in fmt:
                fmt = fmt.rstrip('0').rstrip('.')
            return fmt if fmt != '' else '0'
        except Exception:
            return str(value)

    def _to_csv_number(self, value, decimal_sep: str = ',') -> str:
        """Retourne la repr√©sentation texte du nombre pour le CSV final avec s√©parateur d√©cimal voulu.
        Par d√©faut, utilise la virgule pour √™tre coh√©rent avec les fichiers Sage X3 d'entr√©e.
        """
        s = self._format_number(value)
        if decimal_sep == ',':
            return s.replace('.', ',')
        return s
    
    def process_completed_file(self, session_id: str, completed_file_path: str) -> pd.DataFrame:
        """Traite le fichier template compl√©t√© et calcule les √©carts"""
        try:
            # V√©rifier que le fichier existe et est accessible
            if not os.path.exists(completed_file_path):
                raise FileNotFoundError(f"Fichier compl√©t√© non trouv√©: {completed_file_path}")
            
            # V√©rifier la taille du fichier
            file_size = os.path.getsize(completed_file_path)
            if file_size == 0:
                raise ValueError("Le fichier compl√©t√© est vide")
            
            logger.info(f"Lecture du fichier compl√©t√©: {completed_file_path} ({file_size} bytes)")
            
            # Tentative de lecture avec gestion d'erreur am√©lior√©e
            try:
                completed_df = pd.read_excel(completed_file_path, engine='openpyxl')
            except Exception as excel_error:
                logger.error(f"Erreur lecture Excel avec openpyxl: {excel_error}")
                # Tentative avec un autre moteur
                try:
                    completed_df = pd.read_excel(completed_file_path, engine='xlrd')
                    logger.info("Lecture r√©ussie avec xlrd")
                except Exception as xlrd_error:
                    logger.error(f"Erreur lecture Excel avec xlrd: {xlrd_error}")
                    
                    # Derni√®re tentative : lire le fichier comme binaire et utiliser BytesIO
                    try:
                        import io
                        with open(completed_file_path, 'rb') as f:
                            file_content = f.read()
                        completed_df = pd.read_excel(io.BytesIO(file_content), engine='openpyxl')
                        logger.info("Lecture r√©ussie avec BytesIO")
                    except Exception as bytesio_error:
                        logger.error(f"Erreur lecture avec BytesIO: {bytesio_error}")
                        raise ValueError(f"Impossible de lire le fichier Excel: {excel_error}")        
            
            logger.info(f"Template compl√©t√© charg√©: {len(completed_df)} lignes")
            
            # Sauvegarder le DataFrame compl√©t√©
            session_service.save_dataframe(session_id, "completed_df", completed_df)
            
            # Charger les donn√©es originales
            original_df = session_service.load_dataframe(session_id, "original_df")
            if original_df is None:
                raise ValueError("Donn√©es originales non trouv√©es pour cette session")
            
            # D√©tecter les candidats LOTECART
            lotecart_candidates = lotecart_processor.detect_lotecart_candidates(completed_df)
            if not lotecart_candidates.empty:
                session_service.save_dataframe(session_id, "lotecart_candidates", lotecart_candidates)
                logger.info(f"üéØ {len(lotecart_candidates)} candidats LOTECART d√©tect√©s")
            
            # Calculer les √©carts
            discrepancies = self._calculate_discrepancies(completed_df, original_df)
            session_service.save_dataframe(session_id, "discrepancies_df", discrepancies)
            
            logger.info(f"√âcarts calcul√©s: {len(discrepancies)} lignes avec √©carts")
            return discrepancies
            
        except Exception as e:
            logger.error(f"Erreur traitement fichier compl√©t√©: {e}")
            raise
    
    def _calculate_discrepancies(self, completed_df: pd.DataFrame, original_df: pd.DataFrame) -> pd.DataFrame:
        """Calcule les √©carts entre quantit√©s th√©oriques et r√©elles"""
        discrepancies = []
        
        # Cr√©er un dictionnaire des quantit√©s r√©elles saisies (sans num√©ro de lot)
        real_quantities_dict = {}
        for _, row in completed_df.iterrows():
            key = (
                row["Code Article"],
                row["Num√©ro Inventaire"]
            )
            real_quantities_dict[key] = row["Quantit√© R√©elle"]
        
        # Parcourir les donn√©es originales et calculer les √©carts
        for _, original_row in original_df.iterrows():
            code_article = original_row["CODE_ARTICLE"]
            numero_inventaire = original_row["NUMERO_INVENTAIRE"]
            numero_lot = str(original_row["NUMERO_LOT"]).strip() if pd.notna(original_row["NUMERO_LOT"]) else ""
            quantite_originale = original_row["QUANTITE"]
            statut_article = original_row.get('STATUT')
            unite = original_row.get('UNITE')
            zone_pk = original_row.get('ZONE_PK')
            emplacement = original_row.get('EMPLACEMENT')
            
            key = (code_article, numero_inventaire)
            quantite_reelle_saisie = real_quantities_dict.get(key, 0)
            
            # IMPORTANT: Ne pas calculer la quantit√© corrig√©e ici
            # Elle sera calcul√©e dans distribute_discrepancies selon FIFO/LIFO
            discrepancy_row = {
                'CODE_ARTICLE': code_article,
                'NUMERO_INVENTAIRE': numero_inventaire,
                'NUMERO_LOT': numero_lot,
                'TYPE_LOT': original_row.get('Type_Lot', 'unknown'),
                'QUANTITE_ORIGINALE': quantite_originale,
                'QUANTITE_REELLE_SAISIE_TOTALE': quantite_reelle_saisie,  # Quantit√© totale saisie pour l'article
                'AJUSTEMENT': 0,  # Sera calcul√© dans distribute_discrepancies
                'QUANTITE_CORRIGEE': quantite_originale,  # Initialement = quantit√© originale
                'Date_Lot': original_row.get('Date_Lot'),
                'original_s_line_raw': original_row.get('original_s_line_raw'),
                'STATUT': statut_article,
                'UNITE': unite,
                'ZONE_PK': zone_pk,
                'EMPLACEMENT': emplacement
            }
            
            discrepancies.append(discrepancy_row)
        
        return pd.DataFrame(discrepancies)
    
    def distribute_discrepancies(self, session_id: str, strategy: str = 'FIFO') -> pd.DataFrame:
        """Distribue les √©carts selon la strat√©gie choisie (FIFO/LIFO)"""
        try:
            # Charger les √©carts calcul√©s
            discrepancies_df = session_service.load_dataframe(session_id, "discrepancies_df")
            if discrepancies_df is None:
                raise ValueError("√âcarts non calcul√©s pour cette session")
            
            logger.info(f"üîÑ Distribution des √©carts selon strat√©gie {strategy}")
            
            # Grouper par article et inventaire pour calculer les √©carts totaux
            distributed_rows = []
            
            for (code_article, numero_inventaire, statut_article, unite, zone_pk, emplacement), group in discrepancies_df.groupby(['CODE_ARTICLE', 'NUMERO_INVENTAIRE', 'STATUT', 'UNITE', 'ZONE_PK', 'EMPLACEMENT']):
                # Calculer l'√©cart total pour cet article
                quantite_originale_totale = group['QUANTITE_ORIGINALE'].sum()
                quantite_reelle_saisie = group['QUANTITE_REELLE_SAISIE_TOTALE'].iloc[0]  # M√™me valeur pour tous les lots
                ecart_total = quantite_reelle_saisie - quantite_originale_totale
                
                logger.info(f"üìä Article {code_article} (Inv={numero_inventaire}, Statut={statut_article}, Unite={unite}, Depot={zone_pk}, Empl={emplacement}): Orig={quantite_originale_totale}, Saisie={quantite_reelle_saisie}, √âcart={ecart_total}")
                
                # Trier les lots selon la strat√©gie
                if strategy == 'FIFO':
                    # FIFO: lots les plus anciens en premier (par date puis par num√©ro de lot)
                    sorted_group = group.sort_values(['Date_Lot', 'NUMERO_LOT'], na_position='last')
                elif strategy == 'LIFO':
                    # LIFO: lots les plus r√©cents en premier
                    sorted_group = group.sort_values(['Date_Lot', 'NUMERO_LOT'], ascending=[False, False], na_position='first')
                else:
                    # Par d√©faut, garder l'ordre original
                    sorted_group = group
                
                # Distribuer l'√©cart sur les lots
                ecart_restant = ecart_total
                
                for idx, (row_index, lot_row) in enumerate(sorted_group.iterrows()):
                    quantite_originale_lot = lot_row['QUANTITE_ORIGINALE']
                    
                    if ecart_restant == 0:
                        # Plus d'√©cart √† distribuer, garder la quantit√© originale
                        quantite_corrigee = quantite_originale_lot
                        ajustement_lot = 0
                    elif ecart_restant > 0:
                        # √âcart positif: allouer tout l'√©cart au premier lot selon la strat√©gie
                        if idx == 0:
                            quantite_corrigee = quantite_originale_lot + ecart_restant
                            ajustement_lot = ecart_restant
                            ecart_restant = 0
                        else:
                            quantite_corrigee = quantite_originale_lot
                            ajustement_lot = 0
                    else:
                        # √âcart n√©gatif: retirer du stock
                        if abs(ecart_restant) >= quantite_originale_lot:
                            # On retire tout le stock de ce lot
                            quantite_corrigee = 0
                            ajustement_lot = -quantite_originale_lot
                            ecart_restant += quantite_originale_lot
                        else:
                            # On retire partiellement
                            quantite_corrigee = quantite_originale_lot + ecart_restant  # ecart_restant est n√©gatif
                            ajustement_lot = ecart_restant
                            ecart_restant = 0
                    
                    # Cr√©er la ligne distribu√©e
                    distributed_row = lot_row.copy()
                    distributed_row['AJUSTEMENT'] = ajustement_lot
                    distributed_row['QUANTITE_CORRIGEE'] = quantite_corrigee
                    distributed_row['QUANTITE_REELLE_SAISIE'] = quantite_reelle_saisie  # Garder la saisie totale pour info
                    
                    distributed_rows.append(distributed_row)
                    
                    logger.debug(f"  üì¶ Lot {lot_row['NUMERO_LOT']}: Orig={quantite_originale_lot} ‚Üí Corrig√©e={quantite_corrigee} (Ajust={ajustement_lot})")
                
                # V√©rifier qu'on a bien distribu√© tout l'√©cart
                if abs(ecart_restant) > 0.01:  # Tol√©rance pour les erreurs d'arrondi
                    logger.warning(f"‚ö†Ô∏è √âcart non compl√®tement distribu√© pour {code_article}: {ecart_restant}")
            
            distributed_df = pd.DataFrame(distributed_rows)
            
            # Charger les candidats LOTECART s'ils existent
            lotecart_candidates = session_service.load_dataframe(session_id, "lotecart_candidates")
            
            # Cr√©er les ajustements LOTECART si n√©cessaire
            if lotecart_candidates is not None and not lotecart_candidates.empty:
                original_df = session_service.load_dataframe(session_id, "original_df")
                lotecart_adjustments = lotecart_processor.create_lotecart_adjustments(
                    lotecart_candidates, original_df
                )
                logger.info(f"üéØ {len(lotecart_adjustments)} ajustements LOTECART cr√©√©s")
                
                # Ajouter les ajustements LOTECART
                for lotecart_adj in lotecart_adjustments:
                    lotecart_adj['QUANTITE_REELLE_SAISIE'] = lotecart_adj['QUANTITE_CORRIGEE']
                    distributed_df = pd.concat([distributed_df, pd.DataFrame([lotecart_adj])], ignore_index=True)
            
            # Sauvegarder les donn√©es distribu√©es
            session_service.save_dataframe(session_id, "distributed_df", distributed_df)
            
            # Mettre √† jour les statistiques de session
            stats = self._calculate_session_stats(distributed_df)
            session_service.update_session(session_id, 
                                         strategy_used=strategy,
                                         **stats)
            
            logger.info(f"‚úÖ Distribution termin√©e: {len(distributed_df)} ajustements selon {strategy}")
            return distributed_df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur distribution √©carts: {e}")
            raise
    
    def _calculate_session_stats(self, distributed_df: pd.DataFrame) -> dict:
        """Calcule les statistiques de session"""
        try:
            total_discrepancy = distributed_df['AJUSTEMENT'].sum()
            adjusted_items = len(distributed_df[distributed_df['AJUSTEMENT'] != 0])
            
            return {
                'total_discrepancy': float(total_discrepancy),
                'adjusted_items_count': adjusted_items,
                'status': 'completed'
            }
        except Exception as e:
            logger.error(f"Erreur calcul statistiques: {e}")
            return {'total_discrepancy': 0, 'adjusted_items_count': 0}
    
    def generate_final_file(self, session_id: str) -> str:
        """G√©n√®re le fichier final CSV avec les quantit√©s r√©elles dans la colonne G"""
        try:
            # Charger les donn√©es n√©cessaires
            distributed_df = session_service.load_dataframe(session_id, "distributed_df")
            if distributed_df is None:
                raise ValueError("Donn√©es distribu√©es non trouv√©es")
            
            # R√©cup√©rer les m√©tadonn√©es de session
            session_data = session_service.get_session_data(session_id)
            if not session_data:
                raise ValueError("Session non trouv√©e")
            
            header_lines = json.loads(session_data['header_lines']) if session_data['header_lines'] else []
            
            # Cr√©er le dictionnaire des ajustements avec quantit√©s r√©elles (AVEC num√©ro de lot)
            adjustments_dict = {}
            for _, row in distributed_df.iterrows():
                key = (
                    row["CODE_ARTICLE"],
                    row["NUMERO_INVENTAIRE"],
                    str(row["NUMERO_LOT"]).strip()
                )
                adjustments_dict[key] = {
                    "qte_theo_ajustee": row["QUANTITE_CORRIGEE"],
                    "qte_reelle_saisie": row.get("QUANTITE_REELLE_SAISIE", row["QUANTITE_CORRIGEE"]),  # Nouvelle donn√©e
                    "type_lot": row["TYPE_LOT"]
                }
            
            # G√©n√©rer le nom du fichier final
            original_filename = session_data['original_filename']
            base_name = os.path.splitext(original_filename)[0]
            final_filename = f"{base_name}_corrige_{session_id}.csv"
            final_file_path = os.path.join(config.FINAL_FOLDER, final_filename)
            
            # G√©n√©rer le fichier final
            with open(final_file_path, 'w', encoding='utf-8') as f:
                # √âcrire les en-t√™tes
                for header in header_lines:
                    f.write(header + "\n")
                
                # Traiter les lignes existantes et ajouter les nouvelles lignes LOTECART
                original_df = session_service.load_dataframe(session_id, "original_df")
                max_line_number = 0
                
                # Pr√©parer un mapping d'emplacements modifi√©s depuis le template compl√©t√© (par article/inventaire)
                completed_df = session_service.load_dataframe(session_id, "completed_df")
                emplacement_updates = {}
                if completed_df is not None and not completed_df.empty:
                    for _, row in completed_df.iterrows():
                        try:
                            k = (row.get('Code Article'), row.get('Num√©ro Inventaire'))
                            # Si l'utilisateur a modifi√© l'emplacement, il sera dans la colonne 'Emplacements'
                            new_emplacement = row.get('Emplacements')
                            if k[0] and k[1] and isinstance(new_emplacement, str) and new_emplacement.strip():
                                emplacement_updates[k] = new_emplacement.strip()
                        except Exception:
                            continue
                
                # Traiter chaque ligne originale
                for _, original_row in original_df.iterrows():
                    parts = str(original_row["original_s_line_raw"]).split(";")
                    
                    if len(parts) >= 15:
                        code_article = original_row["CODE_ARTICLE"]
                        numero_inventaire = original_row["NUMERO_INVENTAIRE"]
                        numero_lot = str(original_row["NUMERO_LOT"]).strip()
                        
                        key = (code_article, numero_inventaire, numero_lot)
                        
                        # Mettre √† jour le num√©ro de ligne max
                        try:
                            line_number = int(parts[3])
                            max_line_number = max(max_line_number, line_number)
                        except (ValueError, IndexError):
                            pass
                        
                        # Sauvegarder la quantit√© originale (elle √©tait dans parts[5])
                        quantite_originale = parts[5]
                        
                        # V√©rifier s'il y a un ajustement pour cette ligne
                        if key in adjustments_dict:
                            adjustment_data = adjustments_dict[key]
                            
                            # NOUVELLE LOGIQUE : Inverser les colonnes 5 et 6
                            parts[5] = quantite_originale  # Colonne 5 (F) = Quantit√© originale du fichier initial
                            qte_theo_ajustee = adjustment_data["qte_theo_ajustee"]
                            parts[6] = self._to_csv_number(qte_theo_ajustee)  # Colonne 6 (G) = Quantit√© th√©orique ajust√©e (d√©cimaux support√©s, virgule)
                            
                            # L'indicateur passe √† "2" SEULEMENT si la quantit√© th√©orique ajust√©e est (quasi) 0
                            try:
                                qte_val = float(qte_theo_ajustee)
                            except Exception:
                                # fallback si non convertible: garder indicateur normal
                                qte_val = None
                            if qte_val is not None and abs(qte_val) < self._zero_epsilon:
                                parts[7] = "2"  # Indicateur de compte ajust√© (quantit√© ajust√©e = 0)
                            else:
                                parts[7] = "1"  # Indicateur normal (quantit√© ajust√©e > 0)
                            # Remplacer le num√©ro de lot par "LOTECART" pour les cas LOTECART (existant ou nouveau)
                            if str(adjustment_data.get("type_lot", "")).lower() == "lotecart":
                                if len(parts) > 14:
                                    parts[14] = "LOTECART"
                        else:
                            # Pas d'ajustement, garder les valeurs originales
                            parts[5] = quantite_originale  # Colonne 5 (F) = Quantit√© originale
                            parts[6] = quantite_originale  # Colonne 6 (G) = Quantit√© originale (pas d'ajustement)
                            parts[7] = "1"  # Indicateur normal (pas d'ajustement)
                        
                        # Mise √† jour d'emplacement si modifi√© c√¥t√© template (appliquer √† toutes les lignes du groupe)
                        try:
                            k = (code_article, numero_inventaire)
                            if k in emplacement_updates:
                                # Colonne 10 (EMPLACEMENT) ‚Üí index 9
                                if len(parts) > 9:
                                    parts[9] = emplacement_updates[k]
                        except Exception:
                            pass
                        
                        # √âcrire la ligne
                        f.write(";".join(parts) + "\n")
                
                # Ajouter les nouvelles lignes LOTECART
                lotecart_adjustments = [
                    adj for adj in distributed_df.to_dict('records') 
                    if adj.get('is_new_lotecart', False) and not adj.get('is_existing_update', False)
                ]
                
                if lotecart_adjustments:
                    new_lotecart_lines = lotecart_processor.generate_lotecart_lines(
                        lotecart_adjustments, max_line_number
                    )
                    
                    for line in new_lotecart_lines:
                        # Adapter les lignes LOTECART √† la nouvelle logique des colonnes
                        parts = line.split(";")
                        if len(parts) >= 15:
                            # Pour LOTECART : 
                            # Colonne 5 (F) = 0 (quantit√© originale √©tait 0)
                            # Colonne 6 (G) = quantit√© trouv√©e (quantit√© th√©orique ajust√©e)
                            qte_lotecart = parts[5]  # Quantit√© trouv√©e
                            parts[5] = "0"  # Colonne 5 (F) = Quantit√© originale (√©tait 0 pour LOTECART)
                            # Assurer la virgule comme s√©parateur d√©cimal en sortie
                            parts[6] = str(qte_lotecart).replace('.', ',')  # Colonne 6 (G) = Quantit√© th√©orique ajust√©e
                            line = ";".join(parts)
                        
                        f.write(line + "\n")
                
                logger.info(f"‚úÖ Fichier final g√©n√©r√© avec {len(distributed_df)} ajustements dont {len(lotecart_adjustments)} nouvelles lignes LOTECART")
            
            # Mettre √† jour la session
            session_service.update_session(session_id, 
                                         final_file_path=final_file_path,
                                         status='completed')
            
            logger.info(f"Fichier final g√©n√©r√©: {final_file_path}")
            return final_file_path
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration fichier final: {e}")
            raise

# Instance globale du processeur
processor = InventoryProcessor()

@app.route('/api/health', methods=['GET'])
def health_check():
    """Endpoint de sant√© de l'API"""
    try:
        db_health = db_manager.health_check()
        return jsonify({
            'status': 'healthy' if db_health else 'degraded',
            'database': 'connected' if db_health else 'disconnected',
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/api/upload', methods=['POST'])
@apply_rate_limit('upload')
@handle_api_errors('upload')
def upload_file():
    """Upload et traitement initial du fichier Sage X3"""
    if 'file' not in request.files:
        return jsonify({'error': 'Aucun fichier fourni'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'Nom de fichier vide'}), 400
    
    # Validation s√©curis√©e du fichier
    is_valid, validation_message = FileValidator.validate_file_security(file, config.MAX_FILE_SIZE)
    if not is_valid:
        return jsonify({'error': validation_message}), 400
    
    # Sauvegarde s√©curis√©e
    filename = secure_filename(file.filename)
    session_id = str(uuid.uuid4())[:8]
    timestamped_filename = f"{session_id}_{filename}"
    file_path = os.path.join(config.UPLOAD_FOLDER, timestamped_filename)
    
    file.save(file_path)
    logger.info(f"Fichier sauvegard√©: {file_path}")
    
    # Cr√©er la session en base
    session_creation_timestamp = datetime.now()
    session_service.create_session(
        id=session_id,
        original_filename=filename,
        original_file_path=file_path,
        status='uploaded'
    )
    
    # Traitement du fichier
    file_extension = os.path.splitext(filename)[1].lower()
    success, result, headers, inventory_date = file_processor.validate_and_process_sage_file(
        file_path, file_extension, session_creation_timestamp
    )
    
    if not success:
        session_service.update_session(session_id, status='error')
        return jsonify({'error': result}), 400
    
    # Sauvegarder les donn√©es originales
    session_service.save_dataframe(session_id, "original_df", result)
    
    # Agr√©gation des donn√©es
    aggregated_df = file_processor.aggregate_data(result)
    session_service.save_dataframe(session_id, "aggregated_df", aggregated_df)
    
    # G√©n√©ration du template
    template_path = file_processor.generate_template(aggregated_df, session_id, config.PROCESSED_FOLDER)
    
    # Mise √† jour de la session
    session_service.update_session(
        session_id,
        template_file_path=template_path,
        inventory_date=inventory_date,
        nb_articles=len(aggregated_df),
        nb_lots=len(result),
        total_quantity=float(result['QUANTITE'].sum()),
        status='template_generated',
        header_lines=json.dumps(headers)
    )
    
    corrections = {}
    invalid_cnt = getattr(file_processor, '_last_invalid_qty_count', 0)
    if invalid_cnt:
        corrections['invalid_quantity_replacements'] = invalid_cnt
        corrections['note'] = 'Valeurs QUANTITE invalides remplac√©es par 0 (configurable)'
    
    return jsonify({
        'message': 'Fichier trait√© avec succ√®s',
        'session_id': session_id,
        'template_url': f'/api/download/template/{session_id}',
        'stats': {
            'nb_articles': len(aggregated_df),
            'total_quantity': float(result['QUANTITE'].sum()),
            'nb_lots': len(result),
            'inventory_date': inventory_date.isoformat() if inventory_date else None
        },
        'corrections': corrections or None
    })

@app.route('/api/process', methods=['POST'])
@apply_rate_limit('upload')
@handle_api_errors('process')
def process_completed_file():
    """Traite le fichier template compl√©t√©"""
    if 'file' not in request.files:
        return jsonify({'error': 'Aucun fichier fourni'}), 400
    
    session_id = request.form.get('session_id')
    if not session_id:
        return jsonify({'error': 'ID de session manquant'}), 400
    
    strategy = request.form.get('strategy', 'FIFO')
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'Nom de fichier vide'}), 400
    
    # Validation du fichier compl√©t√©
    logger.info(f"D√©but validation fichier compl√©t√©: {file.filename}")
    is_valid, validation_message, validation_errors = file_processor.validate_completed_template(file)
    logger.info(f"R√©sultat validation: valid={is_valid}, message={validation_message}")
    
    if not is_valid:
        logger.error(f"Validation √©chou√©e: {validation_message}, erreurs: {validation_errors}")
        return jsonify({
            'error': validation_message,
            'details': validation_errors
        }), 400
    
    # Sauvegarde du fichier compl√©t√© avec validation
    completed_filename = f"completed_{session_id}_{secure_filename(file.filename)}"
    completed_file_path = os.path.join(config.PROCESSED_FOLDER, completed_filename)
    
    try:
        # Diagnostic du fichier avant sauvegarde
        file.seek(0)
        file_content = file.read()
        file.seek(0)
        
        logger.info(f"Fichier re√ßu: {file.filename}, taille: {len(file_content)} bytes")
        
        # Sauvegarde avec gestion d'erreur am√©lior√©e
        try:
            file.save(completed_file_path)
        except Exception as save_error:
            logger.error(f"Erreur lors de la sauvegarde: {save_error}")
            # Tentative de sauvegarde alternative
            with open(completed_file_path, 'wb') as f:
                f.write(file_content)
            logger.info("Sauvegarde alternative r√©ussie")
        
        # V√©rifier que le fichier existe
        if not os.path.exists(completed_file_path):
            raise FileNotFoundError("Fichier non sauvegard√© correctement")
        
        file_size = os.path.getsize(completed_file_path)
        if file_size == 0:
            raise ValueError("Fichier sauvegard√© vide")
        
        logger.info(f"Fichier sauvegard√©: {completed_file_path} ({file_size} bytes)")
        
        # Attendre un peu pour que le syst√®me de fichiers se synchronise
        import time
        time.sleep(0.1)
        
        # Test de lecture avec diagnostic d√©taill√©
        try:
            # Essayer d'abord avec openpyxl
            test_df = pd.read_excel(completed_file_path, engine='openpyxl', nrows=1)
            logger.info(f"Fichier valid√© avec openpyxl: {len(test_df.columns)} colonnes")
        except Exception as openpyxl_error:
            logger.warning(f"√âchec lecture avec openpyxl: {openpyxl_error}")
            try:
                # Essayer avec xlrd
                test_df = pd.read_excel(completed_file_path, engine='xlrd', nrows=1)
                logger.info(f"Fichier valid√© avec xlrd: {len(test_df.columns)} colonnes")
            except Exception as xlrd_error:
                logger.error(f"√âchec lecture avec xlrd: {xlrd_error}")
                
                # Diagnostic avanc√© du fichier
                try:
                    import zipfile
                    with zipfile.ZipFile(completed_file_path, 'r') as zip_ref:
                        file_list = zip_ref.namelist()
                        logger.info(f"Contenu ZIP du fichier Excel: {file_list}")
                except Exception as zip_error:
                    logger.error(f"Fichier n'est pas un ZIP valide: {zip_error}")
                
                # Supprimer le fichier corrompu
                if os.path.exists(completed_file_path):
                    os.remove(completed_file_path)
                
                raise ValueError(f"Fichier Excel corrompu. Erreurs: openpyxl={openpyxl_error}, xlrd={xlrd_error}")
            
    except Exception as save_error:
        logger.error(f"Erreur sauvegarde fichier compl√©t√©: {save_error}")
        return jsonify({'error': f'Erreur sauvegarde fichier: {save_error}'}), 500
    
    # Traitement
    discrepancies_df = processor.process_completed_file(session_id, completed_file_path)
    distributed_df = processor.distribute_discrepancies(session_id, strategy)
    final_file_path = processor.generate_final_file(session_id)
    
    # Mise √† jour de la session
    session_service.update_session(
        session_id,
        completed_file_path=completed_file_path,
        final_file_path=final_file_path
    )
    
    # Calcul des statistiques finales
    total_discrepancy = distributed_df['AJUSTEMENT'].sum()
    adjusted_items = len(distributed_df[distributed_df['AJUSTEMENT'] != 0])
    
    return jsonify({
        'message': 'Traitement termin√© avec succ√®s',
        'session_id': session_id,
        'final_url': f'/api/download/final/{session_id}',
        'stats': {
            'total_discrepancy': float(total_discrepancy),
            'adjusted_items': adjusted_items,
            'strategy_used': strategy
        }
    })

@app.route('/api/download/<file_type>/<session_id>', methods=['GET'])
@handle_api_errors('download')
def download_file(file_type, session_id):
    """T√©l√©charge un fichier selon son type"""
    session_data = session_service.get_session_data(session_id)
    if not session_data:
        return jsonify({'error': 'Session non trouv√©e'}), 404
    
    if file_type == 'template':
        file_path = session_data['template_file_path']
        if not file_path or not os.path.exists(file_path):
            return jsonify({'error': 'Template non trouv√©'}), 404
        
        return send_file(
            file_path,
            as_attachment=True,
            download_name=os.path.basename(file_path),
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
    
    elif file_type == 'final':
        file_path = session_data['final_file_path']
        if not file_path or not os.path.exists(file_path):
            return jsonify({'error': 'Fichier final non trouv√©'}), 404
        
        return send_file(
            file_path,
            as_attachment=True,
            download_name=os.path.basename(file_path),
            mimetype='text/csv'
        )
    
    else:
        return jsonify({'error': 'Type de fichier non support√©'}), 400

@app.route('/api/sessions', methods=['GET'])
@handle_api_errors('sessions')
def list_sessions():
    """Liste les sessions actives"""
    sessions = session_service.list_sessions()
    return jsonify({'sessions': sessions})

@app.route('/api/sessions/<session_id>', methods=['DELETE'])
@handle_api_errors('delete_session')
def delete_session_endpoint(session_id):
    """Supprime une session"""
    success = session_service.delete_session(session_id)
    if success:
        # Nettoyer aussi les fichiers de donn√©es
        session_service.cleanup_session_data(session_id)
        return jsonify({'message': 'Session supprim√©e avec succ√®s'})
    else:
        return jsonify({'error': 'Session non trouv√©e'}), 404

if __name__ == '__main__':
    # Ce bloc n'est ex√©cut√© que lors d'un lancement direct (python app.py)
    # En production, Gunicorn est le point d'entr√©e et n'ex√©cute pas ce bloc.
    is_debug_mode = os.environ.get('FLASK_ENV') != 'production'
    logger.info(f"D√©marrage de l'application en mode {'debug' if is_debug_mode else 'production'}")
    app.run(debug=is_debug_mode, host='0.0.0.0', port=5000)